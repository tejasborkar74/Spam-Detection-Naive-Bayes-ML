{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open('D:\\\\5th sem\\\\ML\\\\Projects\\\\Dataset\\\\SMSSpamCollection')\n",
    "f1=open('SMSSpamCollection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Convert f1 into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Data Frame\n",
    "\n",
    "\n",
    "#Make df for raw data\n",
    "arr = np.empty((0,2)) \n",
    "for line in f1:\n",
    "    l = len(line)-1\n",
    "    if line[0] == 'h':\n",
    "        text1 = 0\n",
    "        text2 = line[4:l]\n",
    "        arr = np.append(arr, np.array([[text2 , text1]]) , axis = 0)  \n",
    "      \n",
    "    else :\n",
    "        text1 = 1\n",
    "        text2 = line[5:l]\n",
    "        arr = np.append(arr, np.array([[text2 , text1]]) , axis = 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  email  spam\n",
      "0     Go until jurong point, crazy.. Available only ...     0\n",
      "1                         Ok lar... Joking wif u oni...     0\n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
      "3     U dun say so early hor... U c already then say...     0\n",
      "4     Nah I don't think he goes to usf, he lives aro...     0\n",
      "...                                                 ...   ...\n",
      "5570              Will Ã¼ b going to esplanade fr home?     0\n",
      "5571  Pity, * was in mood for that. So...any other s...     0\n",
      "5572  The guy did some bitching but I acted like i'd...     0\n",
      "5573                         Rofl. Its true to its name     0\n",
      "5574                                                        1\n",
      "\n",
      "[5575 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "col = [\"email\" , \"spam\"]\n",
    "df =  pd.DataFrame(data = arr , columns = col)\n",
    "df['spam'] = df['spam'].apply(lambda x: ord(x) - ord('0'))\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5575, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['email', 'spam'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails :  748\n",
      "Number of ham emails  :  4827\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of spam emails : \" ,df[df['spam'] == 1].shape[0])\n",
    "print(\"Number of ham emails  : \",df[df['spam'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sampling\n",
    " \n",
    "### To make the dataset balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>save up to NUMBER on life insurance why spend...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>NUMBER fight the risk of cancer URL NUMBER sli...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>NUMBER fight the risk of cancer URL NUMBER sli...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>adult club offers free membership instant acc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>i thought you might like these NUMBER slim dow...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>abc s good morning america ranks it the NUMBE...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>hyperlink hyperlink hyperlink let mortgage le...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>thank you for shopping with us gifts for all ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>the famous ebay marketing e course learn to s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>hello this is chinese traditional 子 件 NUMBER世...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "2508   save up to NUMBER on life insurance why spend...   1.0\n",
       "2509  NUMBER fight the risk of cancer URL NUMBER sli...   1.0\n",
       "2510  NUMBER fight the risk of cancer URL NUMBER sli...   1.0\n",
       "2511   adult club offers free membership instant acc...   1.0\n",
       "2512  i thought you might like these NUMBER slim dow...   1.0\n",
       "...                                                 ...   ...\n",
       "3010   abc s good morning america ranks it the NUMBE...   1.0\n",
       "3011   hyperlink hyperlink hyperlink let mortgage le...   1.0\n",
       "3012   thank you for shopping with us gifts for all ...   1.0\n",
       "3013   the famous ebay marketing e course learn to s...   1.0\n",
       "3014   hello this is chinese traditional 子 件 NUMBER世...   1.0\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file1 = pd.read_csv('D:\\\\5th sem\\\\ML\\\\Projects\\\\Dataset\\\\spam_or_not_spam.csv')\n",
    "file1=pd.read_csv('spam_or_not_spam.csv')\n",
    "file1 = file1[file1['label'] == 1]\n",
    "file1.columns=['email','spam']\n",
    "\n",
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>photoshop , windows , office . cheap . main tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>looking for medication ? we ` re the best sour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vocable % rnd - word asceticism\\r\\nvcsc - bran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>report 01405 !\\r\\nwffur attion brom est inst s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vic . odin n ^ ow\\r\\nberne hotbox carnal bride...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>pictures\\r\\nstreamlined denizen ajar chased\\r\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>penny stocks are about timing\\r\\nnomad interna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5162</th>\n",
       "      <td>anomaly boys from 3881\\r\\nuosda apaproved mled...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>slutty milf wants to meet you\\r\\ntake that !\\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>important online banking alert\\r\\ndear valued ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "3     photoshop , windows , office . cheap . main tr...     1\n",
       "7     looking for medication ? we ` re the best sour...     1\n",
       "10    vocable % rnd - word asceticism\\r\\nvcsc - bran...     1\n",
       "11    report 01405 !\\r\\nwffur attion brom est inst s...     1\n",
       "13    vic . odin n ^ ow\\r\\nberne hotbox carnal bride...     1\n",
       "...                                                 ...   ...\n",
       "5159  pictures\\r\\nstreamlined denizen ajar chased\\r\\...     1\n",
       "5161  penny stocks are about timing\\r\\nnomad interna...     1\n",
       "5162  anomaly boys from 3881\\r\\nuosda apaproved mled...     1\n",
       "5164  slutty milf wants to meet you\\r\\ntake that !\\r...     1\n",
       "5170  important online banking alert\\r\\ndear valued ...     1\n",
       "\n",
       "[1499 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file2 = pd.read_csv('D:\\\\5th sem\\\\ML\\\\Projects\\\\Dataset\\\\spam_ham_dataset.csv')\n",
    "file2=pd.read_csv('spam_ham_dataset.csv')\n",
    "file2.drop(['Unnamed: 0','label'], axis =1,inplace = True)\n",
    "file2.columns=['email','spam']\n",
    "file2 = file2[file2['spam'] == 1]\n",
    "file2['email']= file2['email'].apply(lambda str : str[9:])\n",
    "\n",
    "file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>pictures\\r\\nstreamlined denizen ajar chased\\r\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>penny stocks are about timing\\r\\nnomad interna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5162</th>\n",
       "      <td>anomaly boys from 3881\\r\\nuosda apaproved mled...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>slutty milf wants to meet you\\r\\ntake that !\\r...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>important online banking alert\\r\\ndear valued ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7573 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     Go until jurong point, crazy.. Available only ...   0.0\n",
       "1                         Ok lar... Joking wif u oni...   0.0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   1.0\n",
       "3     U dun say so early hor... U c already then say...   0.0\n",
       "4     Nah I don't think he goes to usf, he lives aro...   0.0\n",
       "...                                                 ...   ...\n",
       "5159  pictures\\r\\nstreamlined denizen ajar chased\\r\\...   1.0\n",
       "5161  penny stocks are about timing\\r\\nnomad interna...   1.0\n",
       "5162  anomaly boys from 3881\\r\\nuosda apaproved mled...   1.0\n",
       "5164  slutty milf wants to meet you\\r\\ntake that !\\r...   1.0\n",
       "5170  important online banking alert\\r\\ndear valued ...   1.0\n",
       "\n",
       "[7573 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([df,file1,file2],axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails :  2746\n",
      "Number of ham emails  :  4827\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of spam emails : \" ,df[df['spam'] == 1].shape[0])\n",
    "print(\"Number of ham emails  : \",df[df['spam'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7061, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "#New shape of the df\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>pictures\\r\\nstreamlined denizen ajar chased\\r\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>penny stocks are about timing\\r\\nnomad interna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5162</th>\n",
       "      <td>anomaly boys from 3881\\r\\nuosda apaproved mled...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>slutty milf wants to meet you\\r\\ntake that !\\r...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>important online banking alert\\r\\ndear valued ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7061 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     Go until jurong point, crazy.. Available only ...   0.0\n",
       "1                         Ok lar... Joking wif u oni...   0.0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   1.0\n",
       "3     U dun say so early hor... U c already then say...   0.0\n",
       "4     Nah I don't think he goes to usf, he lives aro...   0.0\n",
       "...                                                 ...   ...\n",
       "5159  pictures\\r\\nstreamlined denizen ajar chased\\r\\...   1.0\n",
       "5161  penny stocks are about timing\\r\\nnomad interna...   1.0\n",
       "5162  anomaly boys from 3881\\r\\nuosda apaproved mled...   1.0\n",
       "5164  slutty milf wants to meet you\\r\\ntake that !\\r...   1.0\n",
       "5170  important online banking alert\\r\\ndear valued ...   1.0\n",
       "\n",
       "[7061 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7056</th>\n",
       "      <td>pictures\\r\\nstreamlined denizen ajar chased\\r\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>penny stocks are about timing\\r\\nnomad interna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7058</th>\n",
       "      <td>anomaly boys from 3881\\r\\nuosda apaproved mled...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7059</th>\n",
       "      <td>slutty milf wants to meet you\\r\\ntake that !\\r...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7060</th>\n",
       "      <td>important online banking alert\\r\\ndear valued ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7061 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     Go until jurong point, crazy.. Available only ...   0.0\n",
       "1                         Ok lar... Joking wif u oni...   0.0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   1.0\n",
       "3     U dun say so early hor... U c already then say...   0.0\n",
       "4     Nah I don't think he goes to usf, he lives aro...   0.0\n",
       "...                                                 ...   ...\n",
       "7056  pictures\\r\\nstreamlined denizen ajar chased\\r\\...   1.0\n",
       "7057  penny stocks are about timing\\r\\nnomad interna...   1.0\n",
       "7058  anomaly boys from 3881\\r\\nuosda apaproved mled...   1.0\n",
       "7059  slutty milf wants to meet you\\r\\ntake that !\\r...   1.0\n",
       "7060  important online banking alert\\r\\ndear valued ...   1.0\n",
       "\n",
       "[7061 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "email    0\n",
       "spam     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Missing data (NAN,na,NaN) for each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7056</th>\n",
       "      <td>pictures\\r\\nstreamlined denizen ajar chased\\r\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>penny stocks are about timing\\r\\nnomad interna...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7058</th>\n",
       "      <td>anomaly boys from 3881\\r\\nuosda apaproved mled...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7059</th>\n",
       "      <td>slutty milf wants to meet you\\r\\ntake that !\\r...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7060</th>\n",
       "      <td>important online banking alert\\r\\ndear valued ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7061 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  spam\n",
       "0     Go until jurong point, crazy.. Available only ...   0.0\n",
       "1                         Ok lar... Joking wif u oni...   0.0\n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   1.0\n",
       "3     U dun say so early hor... U c already then say...   0.0\n",
       "4     Nah I don't think he goes to usf, he lives aro...   0.0\n",
       "...                                                 ...   ...\n",
       "7056  pictures\\r\\nstreamlined denizen ajar chased\\r\\...   1.0\n",
       "7057  penny stocks are about timing\\r\\nnomad interna...   1.0\n",
       "7058  anomaly boys from 3881\\r\\nuosda apaproved mled...   1.0\n",
       "7059  slutty milf wants to meet you\\r\\ntake that !\\r...   1.0\n",
       "7060  important online banking alert\\r\\ndear valued ...   1.0\n",
       "\n",
       "[7061 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(email):\n",
    "\n",
    "    #1 remove punctuations\n",
    "    nopunct = [char for char in email if char not in string.punctuation]\n",
    "    nopunct = ''.join(nopunct)#List to string \n",
    "    \n",
    "    #2 convert into lower case\n",
    "    nopunct = nopunct.lower()\n",
    "    \n",
    "    #3 remove stopwords\n",
    "    clean_words = [word for word in nopunct.split() if word not in stopwords.words('english')]\n",
    "    \n",
    "    #4 stemming\n",
    "    for i in range(len(clean_words)):\n",
    "        clean_words[i] = ps.stem(clean_words[i])\n",
    "       \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails :  2543\n",
      "Number of ham emails  :  4518\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of spam emails : \" ,df[df['spam'] == 1].shape[0])\n",
    "print(\"Number of ham emails  : \",df[df['spam'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer=process_email)\n",
    "vocab = vectorizer.fit_transform(df['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16244)\t1\n",
      "  (0, 20356)\t1\n",
      "  (0, 28309)\t1\n",
      "  (0, 10014)\t1\n",
      "  (0, 4632)\t1\n",
      "  (0, 6904)\t1\n",
      "  (0, 24574)\t1\n",
      "  (0, 16552)\t1\n",
      "  (0, 39022)\t1\n",
      "  (0, 21161)\t1\n",
      "  (0, 12494)\t1\n",
      "  (0, 6895)\t1\n",
      "  (0, 8568)\t1\n",
      "  (0, 16401)\t1\n",
      "  (0, 3502)\t1\n",
      "  (0, 38261)\t1\n",
      "  (1, 26297)\t1\n",
      "  (1, 21293)\t1\n",
      "  (1, 20170)\t1\n",
      "  (1, 38669)\t1\n",
      "  (1, 36503)\t1\n",
      "  (1, 26440)\t1\n",
      "  (2, 15174)\t1\n",
      "  (2, 13274)\t2\n",
      "  (2, 768)\t1\n",
      "  :\t:\n",
      "  (7060, 3174)\t1\n",
      "  (7060, 9210)\t2\n",
      "  (7060, 38234)\t1\n",
      "  (7060, 12311)\t1\n",
      "  (7060, 26450)\t6\n",
      "  (7060, 15152)\t1\n",
      "  (7060, 19307)\t1\n",
      "  (7060, 9569)\t2\n",
      "  (7060, 14554)\t1\n",
      "  (7060, 19612)\t1\n",
      "  (7060, 9195)\t1\n",
      "  (7060, 31428)\t1\n",
      "  (7060, 18552)\t1\n",
      "  (7060, 25387)\t1\n",
      "  (7060, 29103)\t1\n",
      "  (7060, 5383)\t1\n",
      "  (7060, 14068)\t1\n",
      "  (7060, 30242)\t1\n",
      "  (7060, 4468)\t1\n",
      "  (7060, 19250)\t1\n",
      "  (7060, 19321)\t1\n",
      "  (7060, 19216)\t1\n",
      "  (7060, 34473)\t1\n",
      "  (7060, 8629)\t3\n",
      "  (7060, 8176)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7061, 41150)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier :\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.X_filtered1 = self.X_train[self.y_train == 1]\n",
    "        self.X_filtered0 = self.X_train[self.y_train == 0]\n",
    "        self.sum_all1 = np.sum(self.X_filtered1[:,:])\n",
    "        self.sum_all0 = np.sum(self.X_filtered0[:,:])\n",
    "            \n",
    "    #label => spam/ham\n",
    "    def prior_prob(self, label):\n",
    "        total = self.X_train.shape[0]\n",
    "        label_total = np.sum(self.y_train == label)\n",
    "        return label_total/total\n",
    "    \n",
    "    #Conditional probability P(Xi/label)\n",
    "    def conditional_prob(self, col_name, col_value, label):\n",
    "        if label == 1 :\n",
    "            X_filtered = self.X_filtered1\n",
    "            numerator = np.sum(X_filtered[:, col_name]) + 1\n",
    "            denominator = self.sum_all1 + self.X_train.shape[1]\n",
    "        else:\n",
    "            X_filtered = self.X_filtered0\n",
    "            numerator = np.sum(X_filtered[:, col_name]) + 1\n",
    "            denominator = self.sum_all0 + self.X_train.shape[1]\n",
    "        \n",
    "        return float(float(numerator)/float(denominator))\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        row = X_test.shape[0]\n",
    "        col = X_test.shape[1]\n",
    "        pp1 = self.prior_prob(1)\n",
    "        pp0 = self.prior_prob(0)\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        for i in range(row):\n",
    "            s_prob =1.0\n",
    "            ns_prob =1.0\n",
    "            #print(i,end=\" \")\n",
    "            \n",
    "            for j in range(col):\n",
    "                if X_test[i][j] > 0:\n",
    "                    s_prob = s_prob*self.conditional_prob(j , X_test[i][j],1);\n",
    "                    ns_prob =ns_prob*self.conditional_prob(j , X_test[i][j],0);\n",
    "                \n",
    "            s_prob= s_prob*pp1\n",
    "            ns_prob = ns_prob*pp0\n",
    "            #print(s_prob , ns_prob)\n",
    "            if s_prob>ns_prob :\n",
    "                pred.append(1)\n",
    "            else :\n",
    "                pred.append(0)\n",
    "        return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusion_mat(Y_test, pred):\n",
    "    TN = 0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    \n",
    "\n",
    "    for i in range(len(Y_test)):\n",
    "        \n",
    "        if pred[i] == 0 and Y_test[i] == 0:\n",
    "            TN=TN+1\n",
    "        elif pred[i] == 1 and Y_test[i] == 1:\n",
    "            TP=TP+1\n",
    "        elif pred[i] == 1 and Y_test[i] == 0:\n",
    "            FP=FP+1\n",
    "        else:\n",
    "            FN=FN+1\n",
    "    \n",
    "    return TN,TP,FN,FP\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(TN,TP,FN,FP):\n",
    "    total = TN+TP+FN+FP\n",
    "    return (TN+TP)/total\n",
    "\n",
    "\n",
    "def precision_score(TN,TP,FN,FP):\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "\n",
    "def recall_score(TN,TP,FN,FP):\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "\n",
    "def f0pt5_score(TN,TP,FN,FP):\n",
    "    p = TP/(TP+FP)\n",
    "    r = TP/(TP+FN)\n",
    "    return  5*p*r/(p+r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(vocab, df['spam'], test_size=0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.toarray()\n",
    "X_train = X_train.toarray()\n",
    "\n",
    "Y_test = np.array([int(i) for i in Y_test])\n",
    "Y_train = np.array([int(i) for i in Y_train])\n",
    "\n",
    "# Y_test = Y_test.reset_index()\n",
    "# Y_train = Y_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1413, 41150)\n",
      "(1413,)\n",
      "(5648, 41150)\n",
      "(5648,)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "------------------------------------------\n",
      "[0 0 1 ... 0 1 1]\n",
      "[0 1 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))\n",
    "print(type(X_train))\n",
    "print(type(Y_test))\n",
    "print(type(Y_train))\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test)\n",
    "print('------------------------------------------')\n",
    "print(Y_train)\n",
    "print(Y_test)\n",
    "# print(X_train)\n",
    "#print(type(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.fit(X_train , Y_train)\n",
    "\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "print(pred)\n",
    "type(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Performance Measures for Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN  842\n",
      "FP  59\n",
      "FN  181\n",
      "TP  331\n"
     ]
    }
   ],
   "source": [
    "#CONFUSION MATRIX\n",
    "TN,TP,FN,FP = confusion_mat(Y_test,pred)\n",
    "\n",
    "print(\"TN \", TN)\n",
    "print(\"FP \", FP)\n",
    "print(\"FN \", FN)\n",
    "print(\"TP \", TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY   :  0.8301486199575372\n",
      "PRECISION  :  0.8487179487179487\n",
      "RECALL     :  0.646484375\n",
      "F0.5_SCORE :  1.834811529933481\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACY   : \",accuracy_score(TN,TP,FN,FP))\n",
    "print(\"PRECISION  : \",precision_score(TN,TP,FN,FP))\n",
    "print(\"RECALL     : \",recall_score(TN,TP,FN,FP))\n",
    "print(\"F0.5_SCORE : \",f0pt5_score(TN,TP,FN,FP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vocab\n",
    "Y=df['spam'].values\n",
    "\n",
    "Y = np.array([int(i) for i in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN  432  TP  171  FN  84  FP  20\n",
      "TN  427  TP  181  FN  73  FP  25\n",
      "TN  432  TP  193  FN  61  FP  20\n",
      "TN  433  TP  142  FN  112  FP  19\n",
      "TN  425  TP  139  FN  115  FP  27\n",
      "TN  434  TP  149  FN  105  FP  18\n",
      "TN  421  TP  147  FN  107  FP  31\n",
      "TN  424  TP  142  FN  112  FP  28\n",
      "TN  418  TP  135  FN  120  FP  33\n",
      "TN  423  TP  138  FN  117  FP  28\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f0pt5 = []\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits =10, random_state = None)\n",
    "skf.get_n_splits(X ,Y)\n",
    "\n",
    "for train_index, test_index in skf.split(vocab , df['spam'].values):\n",
    "    \n",
    "    X1_train , X1_test = X[train_index], X[test_index]\n",
    "    y1_train , y1_test = Y[train_index], Y[test_index]\n",
    "    \n",
    " \n",
    "    X1_test=X1_test.toarray()\n",
    "    X1_train = X1_train.toarray()\n",
    "    \n",
    "    classifier.fit(X1_train,y1_train)\n",
    "    prediction = classifier.predict(X1_test)\n",
    "    \n",
    "    TN,TP,FN,FP = confusion_mat(y1_test,prediction)\n",
    "    \n",
    "    print(\"TN \", TN , \" TP \" , TP , \" FN \" , FN , \" FP \", FP)  \n",
    "\n",
    "    accuracy.append(accuracy_score(TN,TP,FN,FP))\n",
    "  \n",
    "    precision.append(precision_score(TN,TP,FN,FP))\n",
    "\n",
    "    recall.append(recall_score(TN,TP,FN,FP))\n",
    "\n",
    "    f0pt5.append(f0pt5_score(TN,TP,FN,FP))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures of Stratified K fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACIES  :  [0.8528995756718529, 0.8611898016997167, 0.8852691218130312, 0.8144475920679887, 0.7988668555240793, 0.8257790368271954, 0.8045325779036827, 0.8016997167138811, 0.78328611898017, 0.7946175637393768]\n",
      "PRECISIONS  :  [0.8952879581151832, 0.8786407766990292, 0.9061032863849765, 0.8819875776397516, 0.8373493975903614, 0.8922155688622755, 0.8258426966292135, 0.8352941176470589, 0.8035714285714286, 0.8313253012048193]\n",
      "RECALL      :  [0.6705882352941176, 0.7125984251968503, 0.7598425196850394, 0.5590551181102362, 0.547244094488189, 0.5866141732283464, 0.5787401574803149, 0.5590551181102362, 0.5294117647058824, 0.5411764705882353]\n",
      "F0.5_SCORE  :  [1.9170403587443943, 1.9673913043478264, 2.066381156316916, 1.7108433734939756, 1.6547619047619044, 1.7695961995249405, 1.7013888888888886, 1.6745283018867925, 1.595744680851064, 1.6389548693586697]\n"
     ]
    }
   ],
   "source": [
    "print(\"ACCURACIES  : \",accuracy)\n",
    "print(\"PRECISIONS  : \",precision)\n",
    "print(\"RECALL      : \",recall)\n",
    "print(\"F0.5_SCORE  : \",f0pt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY    :  0.8222587960940975\n",
      "PRECISION   :  0.8587618109344097\n",
      "RECALL      :  0.6044326076887447\n",
      "F0.5_SCORE  :  1.7696631038175372\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(\"ACCURACY    : \",mean(accuracy))\n",
    "print(\"PRECISION   : \",mean(precision))\n",
    "print(\"RECALL      : \",mean(recall))\n",
    "print(\"F0.5_SCORE  : \",mean(f0pt5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Other Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN  448  TP  212  FN  43  FP  4\n",
      "TN  451  TP  209  FN  45  FP  1\n",
      "TN  448  TP  229  FN  25  FP  4\n",
      "TN  452  TP  243  FN  11  FP  0\n",
      "TN  448  TP  217  FN  37  FP  4\n",
      "TN  449  TP  220  FN  34  FP  3\n",
      "TN  450  TP  220  FN  34  FP  2\n",
      "TN  452  TP  229  FN  25  FP  0\n",
      "TN  448  TP  224  FN  31  FP  3\n",
      "TN  450  TP  219  FN  36  FP  1\n"
     ]
    }
   ],
   "source": [
    "X=vocab\n",
    "Y=df['spam'].values\n",
    "\n",
    "Y = np.array([int(i) for i in Y])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f0pt5 = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits =10, random_state = None)\n",
    "skf.get_n_splits(X ,Y)\n",
    "\n",
    "for train_index, test_index in skf.split(vocab , df['spam'].values):\n",
    "    \n",
    "    X1_train , X1_test = X[train_index], X[test_index]\n",
    "    y1_train , y1_test = Y[train_index], Y[test_index]\n",
    "    \n",
    " \n",
    "    X1_test=X1_test.toarray()\n",
    "    X1_train = X1_train.toarray()\n",
    "    \n",
    "    lr.fit(X1_train,y1_train)\n",
    "    prediction = lr.predict(X1_test)\n",
    "    \n",
    "    TN,TP,FN,FP = confusion_mat(y1_test,prediction)\n",
    "    \n",
    "    print(\"TN \", TN , \" TP \" , TP , \" FN \" , FN , \" FP \", FP)  \n",
    "\n",
    "    accuracy.append(accuracy_score(TN,TP,FN,FP))\n",
    "  \n",
    "    precision.append(precision_score(TN,TP,FN,FP))\n",
    "\n",
    "    recall.append(recall_score(TN,TP,FN,FP))\n",
    "\n",
    "    f0pt5.append(f0pt5_score(TN,TP,FN,FP))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY    :  0.9514258467530282\n",
      "PRECISION   :  0.9901229409850508\n",
      "RECALL      :  0.8737918789563069\n",
      "F0.5_SCORE  :  2.3199212914363896\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(\"ACCURACY    : \",mean(accuracy))\n",
    "print(\"PRECISION   : \",mean(precision))\n",
    "print(\"RECALL      : \",mean(recall))\n",
    "print(\"F0.5_SCORE  : \",mean(f0pt5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN  452  TP  145  FN  110  FP  0\n",
      "TN  452  TP  153  FN  101  FP  0\n",
      "TN  452  TP  193  FN  61  FP  0\n",
      "TN  452  TP  238  FN  16  FP  0\n",
      "TN  451  TP  195  FN  59  FP  1\n",
      "TN  452  TP  202  FN  52  FP  0\n",
      "TN  452  TP  190  FN  64  FP  0\n",
      "TN  451  TP  198  FN  56  FP  1\n",
      "TN  451  TP  196  FN  59  FP  0\n",
      "TN  451  TP  194  FN  61  FP  0\n"
     ]
    }
   ],
   "source": [
    "X=vocab\n",
    "Y=df['spam'].values\n",
    "\n",
    "Y = np.array([int(i) for i in Y])\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=70,random_state=3)\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f0pt5 = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits =10, random_state = None)\n",
    "skf.get_n_splits(X ,Y)\n",
    "\n",
    "for train_index, test_index in skf.split(vocab , df['spam'].values):\n",
    "    \n",
    "    X1_train , X1_test = X[train_index], X[test_index]\n",
    "    y1_train , y1_test = Y[train_index], Y[test_index]\n",
    "    \n",
    " \n",
    "    X1_test=X1_test.toarray()\n",
    "    X1_train = X1_train.toarray()\n",
    "    \n",
    "    rf.fit(X1_train,y1_train)\n",
    "    prediction = rf.predict(X1_test)\n",
    "    \n",
    "    TN,TP,FN,FP = confusion_mat(y1_test,prediction)\n",
    "    \n",
    "    print(\"TN \", TN , \" TP \" , TP , \" FN \" , FN , \" FP \", FP)  \n",
    "\n",
    "    accuracy.append(accuracy_score(TN,TP,FN,FP))\n",
    "  \n",
    "    precision.append(precision_score(TN,TP,FN,FP))\n",
    "\n",
    "    recall.append(recall_score(TN,TP,FN,FP))\n",
    "\n",
    "    f0pt5.append(f0pt5_score(TN,TP,FN,FP))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY    :  0.9092288366837493\n",
      "PRECISION   :  0.9989872833555533\n",
      "RECALL      :  0.7487802995213834\n",
      "F0.5_SCORE  :  2.130959715587705\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(\"ACCURACY    : \",mean(accuracy))\n",
    "print(\"PRECISION   : \",mean(precision))\n",
    "print(\"RECALL      : \",mean(recall))\n",
    "print(\"F0.5_SCORE  : \",mean(f0pt5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
